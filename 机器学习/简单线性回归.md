### 简单线性回归

#### 原理

我们知道直线的方程基本上是

y = mx + b

其中b是截距，m是直线的斜率。基本上，线性回归算法给出了截距和斜率的最优值。y和x变量保持不变，因为它们是数据特性，不能更改。我们可以控制的值是截距和斜率。根据截距和斜率的值可以有多条直线。基本上，线性回归算法的作用是在数据点上匹配多条线，并返回误差最小的那条线。

同样的概念可以扩展到有两个以上变量的情况。这叫做多元线性回归。例如，考虑一个场景，您必须根据房子的面积、卧室的数量、该地区居民的平均收入、房子的年龄等等来预测房价。在这种情况下，因变量依赖于几个自变量。涉及多个变量的回归模型可以表示为:

$y = b0 + m1b1 + m2b2 + m3b3 + ... ... mnbn$

这是一个超平面的方程。记住，二维线性回归模型是一条直线;在三维中它是一个平面，在三维以上，它是一个超平面



我们应该怎样从一大堆数据里求出回归方程呢？ 假定输入数据存放在矩阵 x 中，而回归系数存放在向量 w 中。那么对于给定的数据 X1，预测结果将会通过 Y = X1^T w 给出。现在的问题是，手里有一些 X 和对应的 y，怎样才能找到 w 呢？一个常用的方法就是找出使误差最小的 w 。这里的误差是指预测 y 值和真实 y 值之间的差值，使用该误差的简单累加将使得正差值和负差值相互抵消，所以我们采用平方误差（实际上就是我们通常所说的最小二乘法）

$\sum_{i=1}^m(y_{i} - x_{i}^Tw)^2$

用矩阵表示还可以写做 $(Y - X_{w})^T(Y-X_{w})$ 。如果对 w 求导，得到 $X^T(Y-X_{w})$令其等于零，

解出 w 如下（具体求导过程为: http://blog.csdn.net/nomadlx53/article/details/50849941 ）:

$\hat w = (X^TX)^{-1}X^TY$

（$A$ 为$m*n$矩阵，则 $A^T$ 为 $n*m$矩阵)

需要对矩阵求逆，因此这个方程只在逆矩阵存在的时候适用，我们在程序代码中对此作出判断。 判断矩阵是否可逆的一个可选方案是:

判断矩阵的行列式是否为 0，若为 0 ，矩阵就不存在逆矩阵，不为 0 的话，矩阵才存在逆矩阵。

#### 工作原理

```
读入数据，将数据特征x、特征标签y存储在矩阵x、y中
验证 x^Tx 矩阵是否可逆
使用最小二乘法求得 回归系数 w 的最佳估计
```

#### 开发流程

```
收集数据: 采用任意方法收集数据
准备数据: 回归需要数值型数据，标称型数据将被转换成二值型数据
分析数据: 绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比
训练算法: 找到回归系数
测试算法: 使用 R^2 或者预测值和数据的拟合度，来分析模型的效果
使用算法: 使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签
```

#### 算法特点

```
优点：结果易于理解，计算上不复杂。
缺点：对非线性的数据拟合不好。
适用于数据类型：数值型和标称型数据。
```



### 局部加权线性回归

线性回归的一个问题是有可能出现欠拟合现象，因为它求的是具有最小均方差的无偏估计。显而易见，如果模型欠拟合将不能取得最好的预测效果。所以有些方法允许在估计中引入一些偏差，从而降低预测的均方误差。

一个方法是局部加权线性回归（Locally Weighted Linear Regression，LWLR）。在这个算法中，我们给预测点附近的每个点赋予一定的权重，然后与 线性回归 类似，在这个子集上基于最小均方误差来进行普通的回归。我们需要最小化的目标函数大致为:

$$ \sum_{i} w(y^{(i)} - \hat y^{(i)})^2$$

目标函数中 w 为权重，不是回归系数。与 kNN 一样，这种算法每次预测均需要事先选取出对应的数据子集。该算法解出回归系数 w 的形式如下:

$\hat w =(X^TWX)^{-1}X^TWy $

其中 W 是一个矩阵，用来给每个数据点赋予权重。$\hat{w}$ 则为回归系数。 这两个是不同的概念，请勿混用。

LWLR 使用 “核”（与支持向量机中的核类似）来对附近的点赋予更高的权重。核的类型可以自由选择，最常用的核就是高斯核，高斯核对应的权重如下: 

[![局部加权线性回归高斯核](https://github.com/apachecn/AiLearning/raw/master/img/ml/8.Regression/LinearR_23.png)](https://github.com/apachecn/AiLearning/blob/master/img/ml/8.Regression/LinearR_23.png)

这样就构建了一个只含对角元素的权重矩阵 **w**，并且点 x 与 x(i) 越近，w(i) 将会越大。上述公式中包含一个需要用户指定的参数 k ，它决定了对附近的点赋予多大的权重，这也是使用 LWLR 时唯一需要考虑的参数，下面的图给出了参数 k 与权重的关系。

![233](\img\LinearR_6.png)

上面的图是 每个点的权重图（假定我们正预测的点是 x = 0.5），最上面的图是原始数据集，第二个图显示了当 k = 0.5 时，大部分的数据都用于训练回归模型；而最下面的图显示当 k=0.01 时，仅有很少的局部点被用于训练回归模型

#### 局部加权线性回归 注意事项

局部加权线性回归也存在一个问题，即增加了计算量，因为它对每个点做预测时都必须使用整个数据集

#### 开发流程

```
收集数据: 采用任意方法收集数据
准备数据: 回归需要数值型数据，标称型数据将被转换成二值型数据
分析数据: 绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比
训练算法: 找到回归系数
测试算法: 使用 rssError()函数 计算预测误差的大小，来分析模型的效果
使用算法: 使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签
```







### 算法评估

对于回归算法，通常使用三个评价指标

#### MAE

平均绝对误差(MAE)是误差绝对值的平均值。计算公式为

$$\frac{1}{n} \sum_{i=1}^n|Actual- Predicted|$$

#### MSE

均方误差(MSE)为均方误差的均值，计算公式为:

$$\frac{1}{n} \sum_{i=1}^n|Actual- Predicted|^2$$

#### RMSE

均方根误差(RMSE)是平方误差均值的平方根,计算公式为:

$$ \sqrt{\frac{1}{n} \sum_{i=1}^n|Actual- Predicted|^2}$$





### 多元线性回归

多元线性回归尝试通过一个线性方程来适配观察数据，这个线性方程式在两个以上（包括两个）的特征和响应之间构建的一个关系。多元线性回归的实现步骤和简单线性回归很相似，在评价部分有所不同。你可以使用它来找出在预测结果上哪个因素影响力最大，以及不同变量是如何相互关联的

$Y = a + b_1x_1 + b_2x_2 ... b_nx_n$

多元线性回归可以被认为是多个常规线性回归模型，因为您只是在比较给定数量的特征之间的相关性。

对于上述方程，假设因变量与自变量之间存在线性关系。这还假设变量/特性都是连续值，而不是离散值。



在机器学习系统中实现线性回归时，变量必须是连续的，而不是绝对的。但是，经常会有包含分类变量而不是连续变量的数据

例如，数据集可以包含特定国家中某些事件的发生。这些国家是分类变量。为了正确地使用线性回归，这些分类变量必须转换成连续变量。

#### 分类变量

在多元回归模型中，当遇到数据集是非数值数据类型时，使用分类数据是一个非常有效的方法。分类数据，是指反映（事物）类别的数据，是离散数据，其数值个数（分类属性）有限（但可能很多）且指之间无序。二分类变量是只存在于两类中的一种。二分变量可以是“是”或“不是”，也可以是白色或黑色。二分类变量很容易转化为连续变量，它们只能被标记为0或1。



#### 名义/序数变量（Nominal/Ordinal Variables）

名义变量和顺序变量是分类变量的类型，值可以属于任意数量的类别。对于有序变量，假设变量有一定的顺序，或者变量的权值应该不同。因此，可以将类别变量转换为连续值，方法是为它们分配从0开始的数字，并一直运行到类别的长度。

在所有三种类型的转换中，将名义变量转换为连续变量是最具挑战性的任务。这是因为名义变量不应该有不同的权重或顺序，假定所有分类变量都有相等的“值”。这意味着您不能简单地将它们从0排序到类别的数量，因为这意味着较早的类别比较晚的类别具有更少的“价值”。

因此，将名义变量转换为连续变量的默认策略是所谓的“一次性编码”，有时也称为“创建虚拟变量”。实际上，您创建了更多的特性或变量来代表数据中的实际类别。one-hot编码的过程意味着创建一个与类别数目相同的数组，并在与相关类别对应的位置用“1”填充它们，在其他位置用“0”填充。



实例1，通Scikit-Learn库预测波士顿房价

首先，我们需要加载数据集。我们使用的是Scikit-Learn库，它预先包装了一些样本数据集。我们将使用的数据集是波士顿住房数据集。该数据集有许多关于波士顿地区房屋的不同特征，如房屋大小、犯罪率、建筑年龄等。目标是根据这些特征来预测房价。



线性回归与逻辑回归的区别如下描述：

（1）线性回归的样本的输出，都是连续值，$ y\in (-\infty ,+\infty )$，而逻辑回归中$y\in (0,1)$，只能取0和1。

（2）对于拟合函数也有本质上的差别：

​	线性回归：$f(x)=\theta ^{T}x=\theta _{1}x _{1}+\theta _{2}x _{2}+...+\theta _{n}x _{n}$

​	逻辑回归：$f(x)=P(y=1|x;\theta )=g(\theta ^{T}x)$，其中，$g(z)=\frac{1}{1+e^{-z}}$

​	可以看出，线性回归的拟合函数，是对f(x)的输出变量y的拟合，而逻辑回归的拟合函数是对为1类样本的概率的拟合。

​	那么，为什么要以1类样本的概率进行拟合呢，为什么可以这样拟合呢？

​	$\theta ^{T}x=0$就相当于是1类和0类的决策边界：

​	当$\theta ^{T}x>0$，则y>0.5；若$\theta ^{T}x\rightarrow +\infty $，则$y \rightarrow 1 $，即y为1类;

​	当$\theta ^{T}x<0$，则y<0.5；若$\theta ^{T}x\rightarrow -\infty $，则$y \rightarrow 0 $，即y为0类;

这个时候就能看出区别，在线性回归中$\theta ^{T}x$为预测值的拟合函数；而在逻辑回归中$\theta ^{T}x$为决策边界。下表2-3为线性回归和逻辑回归的区别。

​	表2-3 线性回归和逻辑回归的区别