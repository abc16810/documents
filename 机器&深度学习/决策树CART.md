#### 决策树CART算法

[CART](https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29)（Classification and Regression Trees （分类和回归树）

分类问题，可以选择GINI，双化或有序双化；
回归问题，可以使用最小二乘偏差（LSD）或最小绝对偏差（LAD）。



#### 分类树的生成

用于处理离散型数据

(1)对每个特征 A，对它的所有可能取值 a，将数据集分为 A＝a，和 A!＝a 两个子集，计算集合 D 的基尼指数：

$Gini(D,A) = \frac{|D_1|}{D}Gini(D_{1}) + \frac{|D_{2}|}{|D|}Gini(D_{2})$

基尼指数$Gini(D)$表示集合D的不确定性，基尼指数Gini(D,A)表示经A=a分割后集合D的不确定性

(2)遍历所有的特征 A，计算其所有可能取值 a 的基尼指数，选择 D 的基尼指数最小值对应的特征及切分点作为最优的划分，将数据分为两个子集。
(3)对上述两个子节点递归调用步骤(1)(2), 直到满足停止条件。
(4)生成 CART 决策树。

#### 其中 GINI 指数：

1、是一种不等性度量；
2、是介于 0~1 之间的数，0-完全相等，1-完全不相等；
3、总体内包含的类别越杂乱，GINI指数就越大 ,样本集合的不确定性也就越大（跟熵的概念很相似）

定义：
分类问题中，假设有 K 个类，样本属于第 k 类的概率为 $P_k$，则概率分布的基尼指数为：

$Gini(p) = \sum_{k=1}^{k}{p_k}(1-p_{k}) = 1 - \sum_{k=1}^{k}{p^2_{k}}$

样本集合 D 的基尼指数为：

$Gini(D) = 1 - \sum_{k=1}^k \left( \frac{|C_k|}{|D|}\right)^2$

其中 $C_k$ 为数据集 D 中属于第 k 类的样本子集。

#### 回归树的生成

用于处理连续型数据

最小二乘偏差（LSD）算法步骤

![22](C:\Users\Administrator\Desktop\机器学习\img\1667471-876dcbb91f67e692.webp.jpg)

#### CART剪枝

预剪枝(Pre-Pruning)和后剪枝(Post-Pruning)

**是为了减少决策树过拟合**，如果每个属性都被考虑，那决策树的叶节点所覆盖的训练样本基本都是“纯”的，这时候的决策树对训练集表现很好，但是对测试集的表现就会比较差。

**决策树很容易发生过拟合，可以改善的方法有：**
1、通过阈值控制终止条件，避免树形结构分支过细。
2、通过对已经形成的决策树进行剪枝来避免过拟合。
3、基于Bootstrap的思想建立随机森林

在决策树学习中将已生成的树进行简化的过程称为剪枝。决策树的剪枝往往通过极小化决策树的损失函数或代价函数来实现。实际上剪枝的过程就是一个动态规划的过程：从叶结点开始，自底向上地对内部结点计算预测误差以及剪枝后的预测误差，如果两者的预测误差是相等或者剪枝后预测误差更小，当然是剪掉的好。但是如果剪枝后的预测误差更大，那就不要剪了。剪枝后，原内部结点会变成新的叶结点，其决策类别由多数表决法决定。不断重复这个过程往上剪枝，直到预测误差最小为止
**代价复杂度剪枝** Cost-Complexity Pruning(CCP) 方法来对 CART 进行剪枝

CCP算法为子树Tt定义了代价和复杂度，以及一个衡量代价与复杂度之间关系的参数α。

代价指的是在剪枝过程中因子树$T_t $被叶节点替代而增加的错分样本;
复杂度表示剪枝后子树$T_t $减少的叶结点数;
α则表示剪枝后树的复杂度降低程度与代价间的关系

对于分类回归树中的每一个非叶子节点计算它的表面误差率增益值α。

$a = \frac{R(t) - R(T_{t})}{|N_{T}| - 1}$

$|N_{T}|$是子树中包含的叶子节点个数;

$R(t)$是节点t的误差代价，如果该节点被剪枝;  $R(t) = r(t)*p(t)$

r(t)是节点t的误差率;

p(t)是节点t上的数据占所有数据的比例。

$R(T_t)$是子树$T_{t}$的误差代价，如果该节点不被剪枝。它等于子树$T_t$上所有叶子节点的误差代价之和

CCP算法可以分为两个步骤，
Step 1： 按照上述公式从下到上计算每一个非叶节点的$\alpha$值，然后每一次都剪掉具有最小 $\alpha$值的子树。从而得到一个集合{$T_{0}$,$T_{1}$,$T_{2}$,...,$T_{m}$}    其中$T_{0}$表示完整的决策树   $T_{M}$ 表示根节点

Step 2： 根据真实的错误率在集合{$T_{0}$,$T_{1}$,$T_{2}$,...,$T_{m}$} 选出一个最好的决策树

![223](C:\Users\Administrator\Desktop\机器学习\img\870243-20160820210841703-2083764287.png)



#### 总结

1. 对于拥有大量特征的数据决策树会出现过拟合的现象。获得一个合适的样本比例和特征数量十分重要，因为在高维空间中只有少量的样本的树是十分容易过拟合的
2. 通过 `export` 功能可以可视化您的决策树。使用 `max_depth=3` 作为初始树深度，让决策树知道如何适应您的数据，然后再增加树的深度
3. 请记住，填充树的样本数量会增加树的每个附加级别。使用 `max_depth` 来控制输的大小防止过拟合
4. 通过使用 `min_samples_split`（默认2） 和 `min_samples_leaf` 默认（1） 来控制叶节点上的样本数量。当这个值很小时意味着生成的决策树将会过拟合，然而当这个值很大时将会不利于决策树的对样本的学习。所以尝试 `min_samples_leaf=5` 作为初始值。如果样本的变化量很大，可以使用浮点数作为这两个参数中的百分比。两者之间的主要区别在于 `min_samples_leaf` 保证叶结点中最少的采样数，而 `min_samples_split` 可以创建任意小的叶子，尽管在文献中 `min_samples_split` 更常见
5. 在训练之前平衡您的数据集，以防止决策树偏向于主导类.可以通过从每个类中抽取相等数量的样本来进行类平衡，或者优选地通过将每个类的样本权重 (`sample_weight`) 的和归一化为相同的值。还要注意的是，基于权重的预修剪标准 (`min_weight_fraction_leaf`) 对于显性类别的偏倚偏小，而不是不了解样本权重的标准，如 `min_samples_leaf`
6. 如果样本被加权，则使用基于权重的预修剪标准 `min_weight_fraction_leaf` 来优化树结构将更容易，这确保叶节点包含样本权重的总和的至少一部分
7. 所有的决策树内部使用 `np.float32` 数组 ，如果训练数据不是这种格式，将会复制数据集
8. 如果输入的矩阵X为稀疏矩阵，建议您在调用fit之前将矩阵X转换为稀疏的`csc_matrix` ,在调用predict之前将 `csr_matrix` 稀疏。当特征在大多数样本中具有零值时，与密集矩阵相比，稀疏矩阵输入的训练时间可以快几个数量级